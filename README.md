# ATM: Abstractive Thinking Model

> **"Text should be a visualization of abstract information, not just stored knowledge."**

üìÑ **[Read the Full Proposal (PDF)](./ATM_Abstractive_Thinking_Model_Whitepaper.pdf)**  
*(Click to download the white paper)*

---

**ATM** is a conceptual model designed to conduct "Cognitive Training" by converting images into abstract color information.

*Disclaimer: I am an AI enthusiast, not a professional researcher. The terminology and structure used here may differ from standard academic papers. I appreciate your understanding.*

## üõ†Ô∏è Philosophical Foundation
The conceptual basis of ATM is built upon philosophical thinking, drawing inspiration from:
- **Ludwig Wittgenstein**: The concept of "The limits of language."
- **Daoism**: The philosophy of Yin and Yang, and the evolution of chaos into order.

## üìä Concept Overview

![System Architecture](./picture.png)

ATM is not designed to be a standalone model. Instead, it functions as a **foundational layer** (underlying mechanism) for intelligence. It consists of three main components:
1.  **Base Model**: A model equipped with a neural network.
2.  **The Canvas**: An abstract representation layer.
3.  **Source Image**: The visual input.

### What is the "Canvas"?
The Canvas is a mechanism that transforms a raw image into **Abstract Color Information** using specific algorithms and biological features (such as brainwaves/sensations).

## üéØ The Goal: Checking for "Emergent Regularity"

The ultimate goal of this experiment is to observe if the model spontaneously generates **Regularity** (patterns).

**What is Regularity?**
It implies repetition and certainty. In the context of language, it is similar to how different people, sharing the same language state, use similar words or sentences to describe the same context.

**Why check for this?**
When the model outputs similar "words" (or numerical sequences) for similar objects, it indicates that the model has **simplified** the input for some internal reason (e.g., saving compute or processing pressure), rather than outputting random noise.

If the model regularizes an event, its subsequent responses will also gradually become regularized. Ultimately, this allows the model to **independently create a language** via non-linguistic training.

## üÜö ATM vs. LLM

**The Flaw of LLMs:**
Since LLMs are trained on language from the start, they suffer from the **Symbol Grounding Problem**. They process text at a surface level. When they "learn," they often just store text as data/knowledge without true comprehension.

**The Promise of ATM:**
If the ATM experiment succeeds, I predict it will be able to:
1.  Generate a Canvas from text.
2.  Save both the abstract information and the text, linking them together.
3.  Achieve a state where **Text != Knowledge**, but rather **Text = A Visualization of Abstract Information**.

---

## ü§ù Request for Feedback
This is a rough conceptual introduction. 

I explicitly state that I have not undergone traditional AI curriculum training, and my vocabulary in this field is limited. If there are any ambiguities in the text, please forgive me.

I welcome any technical advice or critiques on this architecture.

## ‚ÑπÔ∏è Project Status
This project is currently in the **Conceptual / White Paper** phase. No executable code is provided yet. It serves as a thought experiment regarding the future architecture of AGI.

## üì¨ Contact
You can reach me via X (Twitter) for discussion: **@J_monclare**

---
